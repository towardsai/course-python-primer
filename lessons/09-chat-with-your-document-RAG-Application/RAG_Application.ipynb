{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio==5.17.0 openai==1.57.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_PLSBtu0LHpG",
        "outputId": "27752025-7f57-4da0-ccdd-56eb60da7ad2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "n5dE5pffFmyP",
        "outputId": "99aaa1f1-9de6-4b32-a114-2dcd42f698d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6309bf6f88235fd053.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6309bf6f88235fd053.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from typing import List, Dict\n",
        "import gradio as gr\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "#######################################################\n",
        "# Sample documents with titles\n",
        "#######################################################\n",
        "SAMPLE_DOCUMENTS = [\n",
        "    {\"title\": \"Doc1: DeepSeek R1\", \"content\": \"\"\"Doc1: 1. Introduction: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n",
        "        NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section. 2. Model Summary: Post-Training: Large-Scale Reinforcement Learning on the Base Model. We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\"\"\"},\n",
        "    {\"title\": \"Doc2: OpenAI o1\", \"content\": \"\"\"Doc2: OpenAI o1 is a generative pre-trained transformer (GPT). A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time \"thinking\" before it\n",
        "      answers, making it better at complex reasoning tasks, science and programming than GPT-4o.[1] The full version was released to ChatGPT users on December 5, 2024\"\"\"},\n",
        "    {\"title\": \"Doc3: Python Programming\", \"content\": \"\"\"Doc3: The Python programming language emphasizes readability and simplicity.\n",
        "    It is widely used for data analysis, web development, and artificial intelligence.\n",
        "    Python's ecosystem includes many libraries for scientific computing.\"\"\"},\n",
        "    {\"title\": \"Doc4: Healthy Lifestyle\", \"content\": \"\"\"Doc4: A balanced diet contains fruits, vegetables, protein sources, and whole grains.\n",
        "    Consistent exercise can help maintain healthy body weight.\n",
        "    Both nutrition and exercise are vital components of a healthy lifestyle.\"\"\"},\n",
        "    {\"title\": \"Doc5: Photosynthesis\", \"content\": \"\"\"Doc5: The process of photosynthesis converts light energy into chemical energy in plants.\n",
        "    Chlorophyll molecules absorb sunlight.\n",
        "    Water, carbon dioxide, and sunlight are used to produce glucose and oxygen.\"\"\"}\n",
        "]\n",
        "\n",
        "#######################################################\n",
        "# Our OOP class for Document-based QA (RAG)\n",
        "#######################################################\n",
        "class RAGChatBot:\n",
        "    \"\"\"\n",
        "    RAGChatBot manages:\n",
        "    - Document text, titles, & embeddings\n",
        "    - A method to compute similarity\n",
        "    - A method to retrieve top docs\n",
        "    - A method to query the LLM with a custom prompt\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "\n",
        "    def embed_documents(self, docs: List[Dict[str, str]]) -> None:\n",
        "        \"\"\"\n",
        "        Embeds each document's content using OpenAI embeddings and stores results.\n",
        "        \"\"\"\n",
        "        self.documents = docs\n",
        "        self.embeddings = []\n",
        "        for doc in docs:\n",
        "            response = self.client.embeddings.create(\n",
        "                input=doc[\"content\"],\n",
        "                model=\"text-embedding-3-small\"\n",
        "            )\n",
        "            embedding_vector = response.data[0].embedding\n",
        "            self.embeddings.append(embedding_vector)\n",
        "\n",
        "    def save_embeddings_to_json(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Saves documents and their embeddings to a JSON file.\n",
        "        \"\"\"\n",
        "        data_to_save = []\n",
        "        for doc, embedding in zip(self.documents, self.embeddings):\n",
        "            data_to_save.append({\n",
        "                \"title\": doc[\"title\"],\n",
        "                \"content\": doc[\"content\"],\n",
        "                \"embedding\": embedding\n",
        "            })\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data_to_save, f)\n",
        "\n",
        "    def load_embeddings_from_json(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Loads documents and embeddings from a JSON file.\n",
        "        \"\"\"\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            loaded_data = json.load(f)\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "        for item in loaded_data:\n",
        "            self.documents.append({\"title\": item[\"title\"], \"content\": item[\"content\"]})\n",
        "            self.embeddings.append(item[\"embedding\"])\n",
        "\n",
        "    def cosine_similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Computes cosine similarity between two vectors.\n",
        "        \"\"\"\n",
        "        dot_product = sum(a * b for a, b in zip(vec_a, vec_b))\n",
        "        norm_a = math.sqrt(sum(a * a for a in vec_a))\n",
        "        norm_b = math.sqrt(sum(b * b for b in vec_b))\n",
        "        if norm_a == 0 or norm_b == 0:\n",
        "            return 0\n",
        "        return dot_product / (norm_a * norm_b)\n",
        "\n",
        "    def retrieve_top_documents(self, query: str, top_k: int = 2) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Embeds the query, compares with document embeddings, returns top_k documents.\n",
        "        \"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=query,\n",
        "            model=\"text-embedding-3-small\"\n",
        "        )\n",
        "        query_vec = response.data[0].embedding\n",
        "        sims = []\n",
        "        for idx, emb in enumerate(self.embeddings):\n",
        "            sim_score = self.cosine_similarity(query_vec, emb)\n",
        "            sims.append((sim_score, idx))\n",
        "        sims.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_docs = [self.documents[i[1]] for i in sims[:top_k]]\n",
        "        return top_docs\n",
        "\n",
        "    def summarize_doc(self, doc_text: str) -> str:\n",
        "        \"\"\"\n",
        "        Generates a one-sentence summary for the given document text.\n",
        "        \"\"\"\n",
        "        summary_prompt = f\"Summarize this document in one short sentence:\\n\\n{doc_text}\"\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Creates a prompt using top documents as context, calls the LLM, and returns the final answer.\n",
        "        \"\"\"\n",
        "        top_docs = self.retrieve_top_documents(query, top_k=top_k)\n",
        "        context_str = \"\\n\\n---\\n\\n\".join(f\"{doc['title']}\\n{doc['content']}\" for doc in top_docs)\n",
        "        prompt = f\"\"\"\n",
        "You are a helpful assistant. Use ONLY the following documents to answer the user query:\n",
        "-----\n",
        "{context_str}\n",
        "-----\n",
        "\n",
        "If the answer is not found in the documents, say: \"I can't find the answer in the documents.\"\n",
        "\n",
        "User query: {query}\n",
        "Answer:\n",
        "        \"\"\".strip()\n",
        "        chat_response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        answer = chat_response.choices[0].message.content\n",
        "        used_titles = [doc[\"title\"] for doc in top_docs]\n",
        "        doc_summaries = [self.summarize_doc(doc[\"content\"]) for doc in top_docs]\n",
        "        summary_str = \"\\n\\nDocument Summaries:\\n\" + \"\\n\".join(f\"{title}: {summary}\" for title, summary in zip(used_titles, doc_summaries))\n",
        "        final_answer = answer + f\"\\n\\n(Used documents: {', '.join(used_titles)})\" + summary_str\n",
        "        return final_answer\n",
        "\n",
        "#######################################################\n",
        "# Building the Gradio Interface\n",
        "#######################################################\n",
        "def main():\n",
        "    EMBEDDINGS_JSON = \"doc_embeddings.json\"\n",
        "    api_key =  \"YOUR_OPENAI_API_KEY\"\n",
        "    rag_bot = RAGChatBot(api_key=api_key)\n",
        "    if not os.path.exists(EMBEDDINGS_JSON):\n",
        "        rag_bot.embed_documents(SAMPLE_DOCUMENTS)\n",
        "        rag_bot.save_embeddings_to_json(EMBEDDINGS_JSON)\n",
        "    else:\n",
        "        rag_bot.load_embeddings_from_json(EMBEDDINGS_JSON)\n",
        "\n",
        "    def ask_question(user_query, k):\n",
        "        return rag_bot.answer_query(user_query, top_k=k)\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chat with Your Documents - RAG Demo\")\n",
        "        user_input = gr.Textbox(label=\"Enter your query about the documents here:\")\n",
        "        num_docs_slider = gr.Slider(minimum=1, maximum=5, step=1, value=2, label=\"Number of documents to retrieve\")\n",
        "        answer_output = gr.Textbox(label=\"Answer:\")\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "        submit_button.click(\n",
        "            fn=ask_question,\n",
        "            inputs=[user_input, num_docs_slider],\n",
        "            outputs=answer_output\n",
        "        )\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAJMsVzWLnch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}